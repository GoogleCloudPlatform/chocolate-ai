{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMsUvoF4BP7Y"
      },
      "source": [
        "### To Do / License\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l85J9wppHuh3"
      },
      "source": [
        "- Currently alot of this is working\n",
        "- Need to have people walk toward a truck (in a thread so we can run the rest of the notebook)\n",
        "- Need to have continuous queries run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQgQkbOvj55d"
      },
      "source": [
        "```\n",
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "```\n",
        "\n",
        "Author: Adam Paternostro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m65vp54BUFRi"
      },
      "source": [
        "### Pip installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MaWM6H5i6rX"
      },
      "outputs": [],
      "source": [
        "# To read/write to/from Kafka\n",
        "import sys\n",
        "\n",
        "# https://kafka-python.readthedocs.io/en/master/index.html\n",
        "!{sys.executable} -m pip install kafka-python\n",
        "\n",
        "# https://docs.confluent.io/platform/current/clients/confluent-kafka-python/html/index.html\n",
        "!{sys.executable} -m pip install confluent-kafka"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmyL-Rg4Dr_f"
      },
      "source": [
        "### Initialize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOYsEVSXp6IP"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import base64\n",
        "\n",
        "import google.auth\n",
        "import google.auth.transport.urllib3\n",
        "import urllib3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMlHl3bnkFPZ"
      },
      "outputs": [],
      "source": [
        "# Set these (run this cell to verify the output)\n",
        "\n",
        "# WARNING: Hardcoded for now\n",
        "# For testing you need (These will be automated with Terraform)\n",
        "# network (vpc-main), kafka-subnet, dataflow-subnet\n",
        "# service principals: dataflow-service-account [Optional: kafka-service-principal for using a service principal]\n",
        "# buckets: data-analytics-preview (for AVRO schema), dataflow-staging-us-central1-756740881369 (for dataflow temp files w/o soft delete on)\n",
        "# BigQuery dataset: chocolate_ai [You need to create this]\n",
        "# BigQuery table: customer_geo_location [This is created for you]\n",
        "\n",
        "bigquery_location = \"us\"\n",
        "region = \"us-central1\"\n",
        "kafka_cluster_name = \"chocolate-ai-kafka-cluster-02\"\n",
        "kafka_topic_name = \"customer-location-topic-02\"\n",
        "dataflow_bucket = \"dataflow-staging-us-central1-756740881369\" # should not have logical delete on\n",
        "dataflow_service_account = \"dataflow-service-account@data-analytics-preview.iam.gserviceaccount.com\" # Needs Role: roles/managedkafka.client\n",
        "bigquery_dataset_name = \"chocolate_ai\"\n",
        "bigquery_streaming_destination_table = \"customer_geo_location\"\n",
        "kafka_subnet = \"kafka-subnet\"\n",
        "dataflow_subnet = \"dataflow-subnet\"\n",
        "\n",
        "# kafka_service_principal_name = \"kafka-service-principal\" # No longer need since using logged in user\n",
        "# kafka_service_principal_email = \"kafka-service-principal-email\" # No longer need since using logged in user\n",
        "\n",
        "# Get some values using gcloud\n",
        "project_id = !(gcloud config get-value project)\n",
        "user = !(gcloud auth list --filter=status:ACTIVE --format=\"value(account)\")\n",
        "\n",
        "if len(project_id) == 0:\n",
        "  raise RuntimeError(f\"project_id is not set: {project_id}\")\n",
        "project_id = project_id[0]\n",
        "\n",
        "if len(user) != 1:\n",
        "  raise RuntimeError(f\"user is not set: {user}\")\n",
        "user = user[0]\n",
        "\n",
        "print(f\"project_id = {project_id}\")\n",
        "print(f\"user = {user}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ6m_wGrK0YG"
      },
      "source": [
        "### Helper Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbOjdSP1kN9T"
      },
      "source": [
        "#### restAPIHelper\n",
        "Calls the Google Cloud REST API using the current users credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40wlwnY4kM11"
      },
      "outputs": [],
      "source": [
        "def restAPIHelper(url: str, http_verb: str, request_body: str) -> str:\n",
        "  \"\"\"Calls the Google Cloud REST API passing in the current users credentials\"\"\"\n",
        "\n",
        "  import requests\n",
        "  import google.auth\n",
        "  import json\n",
        "\n",
        "  # Get an access token based upon the current user\n",
        "  creds, project = google.auth.default()\n",
        "  auth_req = google.auth.transport.requests.Request()\n",
        "  creds.refresh(auth_req)\n",
        "  access_token=creds.token\n",
        "\n",
        "  headers = {\n",
        "    \"Content-Type\" : \"application/json\",\n",
        "    \"Authorization\" : \"Bearer \" + access_token\n",
        "  }\n",
        "\n",
        "  if http_verb == \"GET\":\n",
        "    response = requests.get(url, headers=headers)\n",
        "  elif http_verb == \"POST\":\n",
        "    response = requests.post(url, json=request_body, headers=headers)\n",
        "  elif http_verb == \"PUT\":\n",
        "    response = requests.put(url, json=request_body, headers=headers)\n",
        "  elif http_verb == \"PATCH\":\n",
        "    response = requests.patch(url, json=request_body, headers=headers)\n",
        "  elif http_verb == \"DELETE\":\n",
        "    response = requests.delete(url, headers=headers)\n",
        "  else:\n",
        "    raise RuntimeError(f\"Unknown HTTP verb: {http_verb}\")\n",
        "\n",
        "  if response.status_code == 200:\n",
        "    return json.loads(response.content)\n",
        "    #image_data = json.loads(response.content)[\"predictions\"][0][\"bytesBase64Encoded\"]\n",
        "  else:\n",
        "    error = f\"Error restAPIHelper -> ' Status: '{response.status_code}' Text: '{response.text}'\"\n",
        "    raise RuntimeError(error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkj0u39akoZu"
      },
      "source": [
        "#### createApacheKafkaForBigQueryCluster\n",
        "Creates the cluster if it does not exist.  Waits for it to be created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvhCPD8vkMzV"
      },
      "outputs": [],
      "source": [
        "def createApacheKafkaForBigQueryCluster():\n",
        "  \"\"\"Creates a Apache Kafka For BigQuery Cluster.\"\"\"\n",
        "\n",
        "  # First find the cluster if it exists\n",
        "  # https://cloud.google.com/managed-kafka/docs/reference/rest/v1/projects.locations.clusters/list\n",
        "\n",
        "  url = f\"https://managedkafka.googleapis.com/v1/projects/{project_id}/locations/{region}/clusters\"\n",
        "\n",
        "  # Gather existing clusters\n",
        "  json_result = restAPIHelper(url, \"GET\", None)\n",
        "  print(f\"createApacheKafkaForBigQueryCluster (GET) json_result: {json_result}\")\n",
        "\n",
        "  # Test to see if cluster exists, if so return\n",
        "  if \"clusters\" in json_result:\n",
        "    for item in json_result[\"clusters\"]:\n",
        "      print(f\"Apache Kafka for BigQuery: {item['name']}\")\n",
        "      # \"projects/data-analytics-preview/locations/us-central1/clusters/kafka-cluster\"\n",
        "      if item[\"name\"] == f\"projects/{project_id}/locations/{region}/clusters/{kafka_cluster_name}\":\n",
        "        print(\"Apache Kafka for BigQuery already exists\")\n",
        "        return f\"projects/{project_id}/locations/{region}/clusters/{kafka_cluster_name}\"\n",
        "\n",
        "  # Create Apache Kafka For BigQuery Cluster\n",
        "  # https://cloud.google.com/managed-kafka/docs/reference/rest/v1/projects.locations.clusters/create\n",
        "  print(\"Creating Apache Kafka For BigQuery Cluster\")\n",
        "\n",
        "  url = f\"https://managedkafka.googleapis.com/v1/projects/{project_id}/locations/{region}/clusters?clusterId={kafka_cluster_name}\"\n",
        "\n",
        "  request_body = {\n",
        "      \"capacityConfig\": {\n",
        "        \"vcpuCount\": \"3\",\n",
        "        \"memoryBytes\": \"3221225472\"\n",
        "      },\n",
        "      \"gcpConfig\": {\n",
        "          \"accessConfig\": {\n",
        "              \"networkConfigs\": {\n",
        "                  \"subnet\": f\"projects/{project_id}/regions/{region}/subnetworks/{kafka_subnet}\"\n",
        "                  }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "  json_result = restAPIHelper(url, \"POST\", request_body)\n",
        "\n",
        "  name = json_result[\"name\"]\n",
        "  done = json_result[\"done\"]\n",
        "  print(\"Apache Kafka for BigQuery created: \", name)\n",
        "  return f\"projects/{project_id}/locations/{region}/clusters/{kafka_cluster_name}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo38SP8YuFdb"
      },
      "source": [
        "#### waitForApacheKafkaForBigQueryCluster\n",
        "Loops until cluster is created"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viQtCUXXoxih"
      },
      "outputs": [],
      "source": [
        "def waitForApacheKafkaForBigQueryCluster(operation):\n",
        "  \"\"\"\n",
        "  Waits for an Apache Kafka For BigQuery Cluster to be Created.\n",
        "\n",
        "  opertion:\n",
        "    projects/data-analytics-preview/locations/us-central1/operations/operation-1723064212031-61f1e264889a9-9e3a863b-90613855\n",
        "  \"\"\"\n",
        "\n",
        "  url = f\"https://managedkafka.googleapis.com/v1/{operation}\"\n",
        "  max_retries = 100\n",
        "  attempt = 0\n",
        "\n",
        "  while True:\n",
        "    # Gather existing connections\n",
        "    json_result = restAPIHelper(url, \"GET\", None)\n",
        "    print(f\"waitForApacheKafkaForBigQueryCluster (GET) json_result: {json_result}\")\n",
        "\n",
        "    # Test to see if connection exists, if so return\n",
        "    if \"state\" in json_result:\n",
        "      if json_result[\"state\"] == \"ACTIVE\":\n",
        "        print(\"Apache Kafka for BigQuery Cluster created\")\n",
        "        return None\n",
        "\n",
        "    # Wait for 10 seconds\n",
        "    attempt += 1\n",
        "    if attempt > max_retries:\n",
        "      raise RuntimeError(\"Apache Kafka for BigQuery Cluster not created\")\n",
        "    time.sleep(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUwyHLMhuYws"
      },
      "source": [
        "#### deleteApacheKafkaForBigQueryCluster\n",
        "Delete the cluster if exists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYli-CSOkMuN"
      },
      "outputs": [],
      "source": [
        "def deleteApacheKafkaForBigQueryCluster():\n",
        "  \"\"\"Deletes a Apache Kafka For BigQuery Cluster.\"\"\"\n",
        "\n",
        "  # First find the cluster if it exists\n",
        "  # https://cloud.google.com/managed-kafka/docs/reference/rest/v1/projects.locations.clusters/list\n",
        "\n",
        "  url = f\"https://managedkafka.googleapis.com/v1/projects/{project_id}/locations/{region}/clusters\"\n",
        "\n",
        "  # Gather existing clusters\n",
        "  json_result = restAPIHelper(url, \"GET\", None)\n",
        "  print(f\"createApacheKafkaForBigQueryCluster (GET) json_result: {json_result}\")\n",
        "  found = False\n",
        "\n",
        "  # Test to see if cluster, if so then delete\n",
        "  if \"clusters\" in json_result:\n",
        "    for item in json_result[\"clusters\"]:\n",
        "      print(f\"Apache Kafka for BigQuery: {item['name']}\")\n",
        "      # \"projects/data-analytics-preview/locations/us-central1/clusters/kafka-cluster\"\n",
        "      if item[\"name\"] == f\"projects/{project_id}/locations/{region}/clusters/{kafka_cluster_name}\":\n",
        "        print(\"Apache Kafka for BigQuery  exists\")\n",
        "        found = True\n",
        "        break\n",
        "\n",
        "  if found == False:\n",
        "    print(\"Apache Kafka for BigQuery does not exist\")\n",
        "    return None\n",
        "\n",
        "  # Create Apache Kafka For BigQuery Cluster\n",
        "  # https://cloud.google.com/managed-kafka/docs/reference/rest/v1/projects.locations.clusters/delete\n",
        "  print(\"Deleting Apache Kafka For BigQuery Cluster\")\n",
        "\n",
        "  url = f\"https://managedkafka.googleapis.com/v1/projects/{project_id}/locations/{region}/clusters/{kafka_cluster_name}\"\n",
        "\n",
        "  json_result = restAPIHelper(url, \"DELETE\", request_body={})\n",
        "\n",
        "  print(\"Apache Kafka for BigQuery deleted\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCX1j-VKuecW"
      },
      "source": [
        "#### createApacheKafkaForBigQueryTopic\n",
        "Create the topic if not exists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0PlAgsNkMrl"
      },
      "outputs": [],
      "source": [
        "def createApacheKafkaForBigQueryTopic():\n",
        "  \"\"\"Creates a Apache Kafka For BigQuery Topic.\"\"\"\n",
        "\n",
        "  # First find the topic if it exists\n",
        "  # https://cloud.google.com/managed-kafka/docs/reference/rest/v1/projects.locations.clusters.topics/list\n",
        "\n",
        "  url = f\"https://managedkafka.googleapis.com/v1/projects/{project_id}/locations/{region}/clusters/{kafka_cluster_name}/topics\"\n",
        "\n",
        "  # Gather existing clusters\n",
        "  json_result = restAPIHelper(url, \"GET\", None)\n",
        "  print(f\"createApacheKafkaForBigQueryCluster (GET) json_result: {json_result}\")\n",
        "\n",
        "  # Test to see if cluster exists, if so return\n",
        "  if \"topics\" in json_result:\n",
        "    for item in json_result[\"topics\"]:\n",
        "      print(f\"Apache Kafka for BigQuery Topic: {item['name']}\")\n",
        "      # \"projects/data-analytics-preview/locations/us-central1/clusters/kafka-cluster\"\n",
        "      if item[\"name\"] == f\"projects/{project_id}/locations/{region}/clusters/{kafka_cluster_name}/topics/{kafka_topic_name}\":\n",
        "        print(\"Apache Kafka for BigQuery Topic already exists\")\n",
        "        return None\n",
        "\n",
        "\n",
        "  # Create Apache Kafka For BigQuery Topic\n",
        "  # https://cloud.google.com/managed-kafka/docs/reference/rest/v1/projects.locations.clusters.topics/create\n",
        "  print(\"Creating Apache Kafka For BigQuery Topic\")\n",
        "\n",
        "  url = f\"https://managedkafka.googleapis.com/v1/projects/{project_id}/locations/{region}/clusters/{kafka_cluster_name}/topics?topicId={kafka_topic_name}\"\n",
        "\n",
        "  request_body = {\n",
        "      \"partition_count\"    : 2,\n",
        "      \"replication_factor\" : 3\n",
        "    }\n",
        "\n",
        "  json_result = restAPIHelper(url, \"POST\", request_body)\n",
        "\n",
        "  name = json_result[\"name\"]\n",
        "  print(\"Apache Kafka for BigQuery Topic created: \", name)\n",
        "  return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A84k91wbujTD"
      },
      "source": [
        "### Create Apache Kafka for BigQuery Cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9qfeFo-Lksx"
      },
      "source": [
        "Create the cluster and the topic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1AIo1Z5uLKm"
      },
      "outputs": [],
      "source": [
        "# To see your clusters: https://console.cloud.google.com/managedkafka/clusterList\n",
        "\n",
        "opertion = createApacheKafkaForBigQueryCluster()\n",
        "\n",
        "if opertion is not None:\n",
        "  waitForApacheKafkaForBigQueryCluster(opertion)\n",
        "\n",
        "createApacheKafkaForBigQueryTopic()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNs0zbGokXNQ"
      },
      "source": [
        "### Create BigQuery table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2swqVyBGqmxJ"
      },
      "outputs": [],
      "source": [
        "%%bigquery\n",
        "\n",
        "--DROP TABLE `data-analytics-preview.chocolate_ai.customer_geo_location`;\n",
        "\n",
        "CREATE TABLE IF NOT EXISTS `data-analytics-preview.chocolate_ai.customer_geo_location`\n",
        "(\n",
        "    customer_geo_location_id      STRING,\n",
        "    customer_id               INT64,\n",
        "    event_timestamp_millis    INT64,\n",
        "    prior_latitude            FLOAT64,\n",
        "    prior_longitude           FLOAT64,\n",
        "    current_latitude          FLOAT64,\n",
        "    current_longitude         FLOAT64,\n",
        "\n",
        "    debug_destination_latitude  FLOAT64,\n",
        "    debug_destination_longitude FLOAT64,\n",
        "    debug_walking_speed_mps     FLOAT64,\n",
        "    debug_map_url               STRING\n",
        ")\n",
        "CLUSTER BY customer_id, event_timestamp_millis;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiketZsXkdRe"
      },
      "source": [
        "### Create Avro Schema for Kafka Messages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qejUL7fYLX8n"
      },
      "source": [
        "Used to parse data in BigQuery to seperate fields.  The schema must match your BigQuery Table and will be used by the DataFlow job to seperate the fields into columns within the table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gotxGXfHOwTd"
      },
      "outputs": [],
      "source": [
        "avro_schema = {\n",
        "    'namespace': 'com.databeans.customer_geo_location',\n",
        "    'type': 'record',\n",
        "    'name': 'customer_geo_location',\n",
        "    'fields': [\n",
        "        {'name': 'customer_geo_location_id', 'type': 'string'},\n",
        "        {'name': 'customer_id', 'type': 'int'},\n",
        "        {'name': 'event_timestamp_millis', 'type': 'long'},\n",
        "        {'name': 'prior_latitude', 'type': 'double'},\n",
        "        {'name': 'prior_longitude', 'type': 'double'},\n",
        "        {'name': 'current_latitude', 'type': 'double'},\n",
        "        {'name': 'current_longitude', 'type': 'double'},\n",
        "        {'name': 'debug_destination_latitude', 'type': 'double'},\n",
        "        {'name': 'debug_destination_longitude', 'type': 'double'},\n",
        "        {'name': 'debug_walking_speed_mps', 'type': 'double'},\n",
        "        {'name': 'debug_map_url', 'type': 'string'}\n",
        "    ]\n",
        "}\n",
        "\n",
        "with open('customer_geo_location.avsc', 'w') as out:\n",
        "    json.dump(avro_schema, out, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NT1bz7fF81J5"
      },
      "outputs": [],
      "source": [
        "# Save to storage so dataflow job can see it\n",
        "!gsutil cp customer_geo_location.avsc gs://data-analytics-preview/customer_geo_location.avsc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRt5FC6XDtiw"
      },
      "source": [
        "### DataFlow - Stream Data from Apache Kafka for BigQuery to a BigQuery Table (streaming ingestion)\n",
        "- <font color='red'>**WARNING:**</font> This will create a new job everything this is run.  The notebook will only stop the lastest job, so please check the DataFlow UI to Cancel any additional job.s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TUUHdtlFxnQ"
      },
      "source": [
        "#### createDataflowJobApacheKafkaToBigQuery\n",
        "Creates the DataFlow Job (verifies the job name, must be a new name for each job)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOxdoc3Fwqbo"
      },
      "outputs": [],
      "source": [
        "def createDataflowJobApacheKafkaToBigQuery(jobName):\n",
        "  \"\"\"Creates a DataFlow job to copy data from Apache Kafka for BiqQuery to stream data into a BigQuery Table\"\"\"\n",
        "\n",
        "  # First find the job if it exists\n",
        "  # https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.jobs/list\n",
        "\n",
        "  url = f\"https://dataflow.googleapis.com/v1b3/projects/{project_id}/jobs?location={region}\"\n",
        "\n",
        "  # Gather existing job\n",
        "  json_result = restAPIHelper(url, \"GET\", None)\n",
        "  print(f\"createDataflowJobApacheKafkaToBigQuery (GET) json_result: {json_result}\")\n",
        "\n",
        "  # Test to see if job exists, if so return\n",
        "  if \"jobs\" in json_result:\n",
        "    for item in json_result[\"jobs\"]:\n",
        "      print(f\"DataFlow Job Name: {item['name']}\")\n",
        "      if item[\"name\"] == jobName:\n",
        "        print(f\"DataFlow job already exists with date of {item['currentState']}.  Try a new name.\")\n",
        "        return None\n",
        "\n",
        "  # Create DataFlow Job\n",
        "  # https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.locations.flexTemplates/launch\n",
        "  print(\"Creating DataFlow Job from Flex Template\")\n",
        "\n",
        "  url = f\"https://dataflow.googleapis.com/v1b3/projects/{project_id}/locations/{region}/flexTemplates:launch\"\n",
        "\n",
        "  # Continuous Queries needs useStorageWriteApiAtLeastOnce = True\n",
        "  # https://cloud.google.com/dataflow/docs/guides/templates/provided/kafka-to-bigquery#optional-parameters\n",
        "  #numStorageWriteApiStreams : Specifies the number of write streams, this parameter must be set. Default is 0.\n",
        "  #storageWriteApiTriggeringFrequencySec : Specifies the triggering frequency in seconds, this parameter must be set. Default is 5 seconds.\n",
        "  #useStorageWriteApiAtLeastOnce : This parameter takes effect only if \"Use BigQuery Storage Write API\" is enabled. If enabled the at-least-once semantics will be used for Storage Write API, otherwise exactly-once semantics will be used. Defaults to: false.\n",
        "\n",
        "  request_body = {\n",
        "    \"launch_parameter\": {\n",
        "        \"jobName\": jobName,\n",
        "        \"containerSpecGcsPath\": \"gs://dataflow-templates-us-central1/latest/flex/Kafka_to_BigQuery_Flex\",\n",
        "        \"parameters\": {\n",
        "            \"readBootstrapServerAndTopic\": f\"projects/{project_id}/locations/{region}/clusters/{kafka_cluster_name}/topics/{kafka_topic_name}\",\n",
        "            \"persistKafkaKey\": \"false\",\n",
        "            \"writeMode\": \"SINGLE_TABLE_NAME\",\n",
        "            \"numStorageWriteApiStreams\": \"2\",\n",
        "            \"useStorageWriteApiAtLeastOnce\": \"true\",\n",
        "            \"storageWriteApiTriggeringFrequencySec\": \"5\",\n",
        "            \"enableCommitOffsets\": \"false\",\n",
        "            \"kafkaReadOffset\": \"latest\",\n",
        "            \"kafkaReadAuthenticationMode\": \"APPLICATION_DEFAULT_CREDENTIALS\",\n",
        "            \"messageFormat\": \"JSON\",\n",
        "            \"useBigQueryDLQ\": \"false\",\n",
        "            \"stagingLocation\": f\"gs://{dataflow_bucket}/staging\",\n",
        "            \"autoscalingAlgorithm\": \"NONE\",\n",
        "            \"serviceAccount\": dataflow_service_account,\n",
        "            \"labels\": \"{\\\"goog-dataflow-provided-template-type\\\":\\\"flex\\\",\\\"goog-dataflow-provided-template-name\\\":\\\"kafka_to_bigquery_flex\\\",\\\"goog-dataflow-provided-template-version\\\":\\\"2024-07-16-00_rc00\\\"}\",\n",
        "            \"outputTableSpec\": f\"{project_id}:{bigquery_dataset_name}.{bigquery_streaming_destination_table}\"\n",
        "        },\n",
        "        \"environment\": {\n",
        "            \"numWorkers\": 2,\n",
        "            \"tempLocation\": f\"gs://{dataflow_bucket}/tmp\",\n",
        "            \"subnetwork\": f\"regions/{region}/subnetworks/{dataflow_subnet}\",\n",
        "            \"enableStreamingEngine\": True,\n",
        "            \"additionalExperiments\": [\n",
        "                \"enable_streaming_engine\"\n",
        "            ],\n",
        "            \"additionalUserLabels\": {}\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "  json_result = restAPIHelper(url, \"POST\", request_body)\n",
        "\n",
        "  job = json_result[\"job\"]\n",
        "  print(\"Apache Kafka for BigQuery created: \", job)\n",
        "  return job"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9p6kRH4F6vv"
      },
      "source": [
        "#### stopDataflowJobApacheKafkaToBigQuery\n",
        "Stops a DataFlow job.  Looks up the ID based upon the name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UudLX-N19wpK"
      },
      "outputs": [],
      "source": [
        "def stopDataflowJobApacheKafkaToBigQuery(jobName):\n",
        "  \"\"\"Stops a DataFlow job to copy data from Apache Kafka for BiqQuery to stream data into a BigQuery Table\"\"\"\n",
        "\n",
        "  # First find the job if it exists\n",
        "  # https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.jobs/list\n",
        "\n",
        "  url = f\"https://dataflow.googleapis.com/v1b3/projects/{project_id}/jobs?location={region}\"\n",
        "\n",
        "  # Gather existing jobs\n",
        "  json_result = restAPIHelper(url, \"GET\", None)\n",
        "  print(f\"stopDataflowJobApacheKafkaToBigQuery (GET) json_result: {json_result}\")\n",
        "  found = False\n",
        "\n",
        "  # Test to see if job exists, if so return\n",
        "  if \"jobs\" in json_result:\n",
        "    for item in json_result[\"jobs\"]:\n",
        "      print(f\"DataFlow Job Name: {item['name']} - {item['currentState']}\")\n",
        "      if item[\"name\"] == jobName and item[\"currentState\"] == \"JOB_STATE_RUNNING\":\n",
        "        jobId = item[\"id\"]\n",
        "        found = True\n",
        "        break\n",
        "\n",
        "  if not found:\n",
        "    print(\"DataFlow not found or is not running.\")\n",
        "    return\n",
        "\n",
        "  # Stop DataFlow Job\n",
        "  # https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.jobs/update\n",
        "  print(\"Stopping DataFlow Job \")\n",
        "\n",
        "  url=f\"https://dataflow.googleapis.com/v1b3/projects/{project_id}/locations/{region}/jobs/{jobId}\"\n",
        "  print(url)\n",
        "\n",
        "  request_body = { \"requestedState\" : \"JOB_STATE_CANCELLED\" }\n",
        "\n",
        "  json_result = restAPIHelper(url, \"PUT\", request_body)\n",
        "\n",
        "  #job = json_result[\"job\"]\n",
        "  print(\"DataFlow Job Stopped\")\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrARsGGPpbsc"
      },
      "source": [
        "#### waitForDataFlowJobToStart\n",
        "Whats for a job to start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTOTajtzpbMR"
      },
      "outputs": [],
      "source": [
        "def waitForDataFlowJobToStart(jobName):\n",
        "  \"\"\"Waits for job to turn to running\"\"\"\n",
        "\n",
        "  # First find the job if it exists\n",
        "  # https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.jobs/list\n",
        "\n",
        "  url = f\"https://dataflow.googleapis.com/v1b3/projects/{project_id}/jobs?location={region}\"\n",
        "\n",
        "  # Gather existing jobs\n",
        "  json_result = restAPIHelper(url, \"GET\", None)\n",
        "  print(f\"stopDataflowJobApacheKafkaToBigQuery (GET) json_result: {json_result}\")\n",
        "  found = False\n",
        "\n",
        "  # Test to see if job exists, if so return\n",
        "  if \"jobs\" in json_result:\n",
        "    for item in json_result[\"jobs\"]:\n",
        "      print(f\"DataFlow Job Name: {item['name']} - {item['currentState']}\")\n",
        "      if item[\"name\"] == jobName:\n",
        "        jobId = item[\"id\"]\n",
        "        found = True\n",
        "        break\n",
        "\n",
        "  if not found:\n",
        "    print(\"DataFlow not found or is not running.\")\n",
        "    return\n",
        "\n",
        "  # Gets the job status\n",
        "  # https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.locations.jobs/get\n",
        "  print(\"Getting DataFlow Job \")\n",
        "  url=f\"https://dataflow.googleapis.com/v1b3/projects/{project_id}/locations/{region}/jobs/{jobId}\"\n",
        "  print(url)\n",
        "\n",
        "  max_retries = 100\n",
        "  attempt = 0\n",
        "\n",
        "  while True:\n",
        "    # Get Job\n",
        "    json_result = restAPIHelper(url, \"GET\", None)\n",
        "\n",
        "    # Test to see if connection exists, if so return\n",
        "    if \"currentState\" in json_result:\n",
        "      print(f\"waitForDataFlowJobToStart (GET) currentState: {json_result['currentState']}\")\n",
        "      if json_result[\"currentState\"] == \"JOB_STATE_RUNNING\":\n",
        "        print(\"DataFlow Job is now running\")\n",
        "        return None\n",
        "    else:\n",
        "      print(f\"waitForDataFlowJobToStart (GET) json_result: {json_result}\")\n",
        "\n",
        "\n",
        "    # Wait for 10 seconds\n",
        "    attempt += 1\n",
        "    if attempt > max_retries:\n",
        "      raise RuntimeError(\"DataFlow Job not created\")\n",
        "    time.sleep(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lacWbLDHGCL4"
      },
      "source": [
        "#### Run the DataFlow Job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5sqN46oxfxy"
      },
      "outputs": [],
      "source": [
        "# The job can take a few minutes to start.  Click the link to see the progress:\n",
        "# https://console.cloud.google.com/dataflow/jobs\n",
        "\n",
        "jobName= f\"kafka-stream-{datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
        "createDataflowJobApacheKafkaToBigQuery(jobName)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zh-F6ofXZWgM"
      },
      "outputs": [],
      "source": [
        "print(f\"https://console.cloud.google.com/dataflow/jobs?project={project_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9biLEhVgq6BJ"
      },
      "outputs": [],
      "source": [
        "waitForDataFlowJobToStart(jobName)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftO94aBR-t1O"
      },
      "source": [
        "### Token Provider / Confluent Provider\n",
        "Use logged in users credentials (versus a service account)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dnv_oh6fPhDq"
      },
      "source": [
        "*   https://cloud.google.com/managed-kafka/docs/authentication-kafka#oauthbearer\n",
        "*   https://github.com/googleapis/managedkafka\n",
        "* https://docs.confluent.io/platform/current/clients/confluent-kafka-python/html/index.html#pythonclient-configuration\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hr_xe7gMHM5V"
      },
      "outputs": [],
      "source": [
        "# This class is used to get the current Application Default Credential (the user in the case of a notebook)\n",
        "# This is then called by the Kafka Config by the Python Kafka Library\n",
        "\n",
        "class TokenProvider(object):\n",
        "\n",
        "  def __init__(self, **config):\n",
        "    self.credentials, _project = google.auth.default()\n",
        "    self.http_client = urllib3.PoolManager()\n",
        "    self.HEADER = json.dumps(dict(typ='JWT', alg='GOOG_OAUTH2_TOKEN'))\n",
        "\n",
        "  def valid_credentials(self):\n",
        "    if not self.credentials.valid:\n",
        "      self.credentials.refresh(google.auth.transport.urllib3.Request(self.http_client))\n",
        "    return self.credentials\n",
        "\n",
        "  def get_jwt(self, creds):\n",
        "    # print(creds.expiry.timestamp())\n",
        "    return json.dumps(\n",
        "        dict(\n",
        "            exp=creds.expiry.timestamp(),\n",
        "            iss='Google',\n",
        "            iat=datetime.datetime.now(datetime.timezone.utc).timestamp(),\n",
        "            scope='kafka',\n",
        "            sub=creds.service_account_email,\n",
        "        )\n",
        "    )\n",
        "\n",
        "  def b64_encode(self, source):\n",
        "    return (\n",
        "        base64.urlsafe_b64encode(source.encode('utf-8'))\n",
        "        .decode('utf-8')\n",
        "        .rstrip('=')\n",
        "    )\n",
        "\n",
        "  def get_kafka_access_token(self, creds):\n",
        "    # print(self.get_jwt(creds))\n",
        "    return '.'.join([\n",
        "      self.b64_encode(self.HEADER),\n",
        "      self.b64_encode(self.get_jwt(creds)),\n",
        "      self.b64_encode(creds.token)\n",
        "    ])\n",
        "\n",
        "  def token(self):\n",
        "    try:\n",
        "      # print(\"TokenProvider.token() called\")\n",
        "      creds = self.valid_credentials()\n",
        "\n",
        "      # Convert to UTC\n",
        "      utc_expiry = creds.expiry.replace(tzinfo=datetime.timezone.utc)\n",
        "      expiry_seconds = (utc_expiry - datetime.datetime.now(datetime.timezone.utc)).total_seconds()\n",
        "\n",
        "      return self.get_kafka_access_token(creds)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      raise Exception(e)\n",
        "\n",
        "  def confluent_token(self):\n",
        "    try:\n",
        "      # print(\"TokenProvider.confluent_token() called\")\n",
        "      creds = self.valid_credentials()\n",
        "\n",
        "      # Convert to UTC\n",
        "      utc_expiry = creds.expiry.replace(tzinfo=datetime.timezone.utc)\n",
        "      expiry_seconds = (utc_expiry - datetime.datetime.now(datetime.timezone.utc)).total_seconds()\n",
        "\n",
        "      return self.get_kafka_access_token(creds), time.time() + expiry_seconds\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      raise Exception(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHxplUmlODR2"
      },
      "outputs": [],
      "source": [
        "# Confluent does not use a TokenProvider, it calls a method\n",
        "def ConfluentTokenProvider(args, config):\n",
        "  \"\"\"Method to get the Confluent Token\"\"\"\n",
        "  t = TokenProvider()\n",
        "  return t.confluent_token()\n",
        "\n",
        "\n",
        "# Print any Confluent errors (for debugging)\n",
        "def ConfluentErrorProvider(e):\n",
        "  print(e)\n",
        "  raise Exception(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9hb5KkiB2pe"
      },
      "outputs": [],
      "source": [
        "# For Debugging - WARNING: Never save these in your output of the notebook!\n",
        "# t = TokenProvider()\n",
        "# t.token()\n",
        "\n",
        "# ConfluentTokenProvider(None, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_x4X8RSNMJt"
      },
      "source": [
        "#### Helper Methods (Fake Data and Callback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0jxPXHjKb94"
      },
      "outputs": [],
      "source": [
        "# This can be used to do a callback when you publish a message\n",
        "# Since we might publish a lot of messages, this is commented out in the Producer code\n",
        "\n",
        "def delivery_callback(err, msg):\n",
        "    if err:\n",
        "        print('ERROR: Message failed delivery: {}'.format(err))\n",
        "    else:\n",
        "        print(\"Produced event to topic {topic}: value = {value:12}\".format(topic=msg.topic(),  value=msg.value().decode('utf-8')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP8XKiCRfa65"
      },
      "source": [
        "### Create customers who are walking around toward the various locations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlDNFYg_b6Do"
      },
      "source": [
        "#### Latitude / Longitude Helper Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITnZPgO_b9m2"
      },
      "source": [
        "##### haversine_distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VItkUgxsiO5C"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def haversine_distance(lat1, lon1, lat2, lon2):\n",
        "  \"\"\"\n",
        "  Calculates the haversine distance between two points on a sphere.\n",
        "\n",
        "  Args:\n",
        "    lat1: Latitude of the first point in radians.\n",
        "    lon1: Longitude of the first point in radians.\n",
        "    lat2: Latitude of the second point in radians.\n",
        "    lon2: Longitude of the second point in radians.\n",
        "\n",
        "  Returns:\n",
        "    The distance between the two points in kilometers.\n",
        "  \"\"\"\n",
        "\n",
        "  # Earth's radius in kilometers\n",
        "  R = 6371\n",
        "\n",
        "  # Convert degrees to radians\n",
        "  lat1 = math.radians(lat1)\n",
        "  lon1 = math.radians(lon1)\n",
        "  lat2 = math.radians(lat2)\n",
        "  lon2 = math.radians(lon2)\n",
        "\n",
        "  # Haversine formula\n",
        "  dlat = lat2 - lat1\n",
        "  dlon = lon2 - lon1\n",
        "  a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
        "  c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
        "  distance = R * c\n",
        "\n",
        "  return distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-QP67qghMrS"
      },
      "outputs": [],
      "source": [
        "# Create 10 people who are in london\n",
        "# Have the walk towards the location location_1_lat_min, london_lat_max = 51.5174328, -0.1219854\n",
        "# When they are within 1 km send them an alery\n",
        "# Only send them the alert once\n",
        "\n",
        "# have people walk at different speeds\n",
        "# The average walking speed for most adults is around 4.8 kilometers per hour"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etIgADeob6FF"
      },
      "source": [
        "##### bounding_box"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzHH2HWuE2d5"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def bounding_box(latitude, longitude, distance_km):\n",
        "  \"\"\"\n",
        "  Calculates the bounding box coordinates for a given latitude, longitude, and distance.\n",
        "\n",
        "  Args:\n",
        "    latitude: Latitude of the center point in decimal degrees.\n",
        "    longitude: Longitude of the center point in decimal degrees.\n",
        "    distance_km: Distance in kilometers for the bounding box.\n",
        "\n",
        "  Returns:\n",
        "    A tuple containing the minimum and maximum latitude and longitude values\n",
        "    (min_lat, max_lat, min_lon, max_lon).\n",
        "  \"\"\"\n",
        "\n",
        "  # Earth's radius in kilometers\n",
        "  earth_radius_km = 6371\n",
        "\n",
        "  # Convert latitude and longitude to radians\n",
        "  lat_rad = math.radians(latitude)\n",
        "  lon_rad = math.radians(longitude)\n",
        "\n",
        "  # Calculate angular radius\n",
        "  angular_radius = distance_km / earth_radius_km\n",
        "\n",
        "  # Calculate bounding box coordinates\n",
        "  min_lat = math.degrees(lat_rad - angular_radius)\n",
        "  max_lat = math.degrees(lat_rad + angular_radius)\n",
        "\n",
        "  # Handle potential issues with longitude calculations near the poles\n",
        "  if abs(lat_rad) > math.pi / 2 - angular_radius:\n",
        "    # Adjust longitude range to cover the entire circle\n",
        "    min_lon = -180\n",
        "    max_lon = 180\n",
        "  else:\n",
        "    min_lon = math.degrees(lon_rad - angular_radius / math.cos(lat_rad))\n",
        "    max_lon = math.degrees(lon_rad + angular_radius / math.cos(lat_rad))\n",
        "\n",
        "  return min_lat, max_lat, min_lon, max_lon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nm_hoP6Xb_AT"
      },
      "source": [
        "#### Kafka Producers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dhBImkEb0q9"
      },
      "source": [
        "##### simulate_walk_open_source_kafka_producer (Open Source Kafka Producer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kV82HHSIP8I"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from geopy.distance import geodesic\n",
        "import urllib.parse\n",
        "import uuid\n",
        "\n",
        "def simulate_walk_open_source_kafka_producer(customer_id, customer_name, starting_latitude, starting_longitude, \\\n",
        "                  ending_latitude, ending_longitude, speed_meters_per_second=1.4, just_one_record = False):\n",
        "    \"\"\"\n",
        "    Simulates a walk from a starting point to an ending point.\n",
        "\n",
        "    Args:\n",
        "        customer_name (str): Name of the person walking.\n",
        "        starting_latitude (float): Starting latitude in degrees.\n",
        "        starting_longitude (float): Starting longitude in degrees.\n",
        "        ending_latitude (float): Ending latitude in degrees.\n",
        "        ending_longitude (float): Ending longitude in degrees.\n",
        "        speed_meters_per_second (float, optional): Walking speed in meters per second. Defaults to 1.4.\n",
        "\n",
        "    Prints information about the walk at regular intervals.\n",
        "    \"\"\"\n",
        "    from kafka import KafkaProducer\n",
        "    # Kafka Producer configuration with OAUTHBEARER authentication\n",
        "    config = {\n",
        "        'bootstrap_servers': f'bootstrap.{kafka_cluster_name}.{region}.managedkafka.{project_id}.cloud.goog',\n",
        "        'security_protocol': 'SASL_SSL',\n",
        "        'sasl_mechanism': 'OAUTHBEARER',\n",
        "        'sasl_oauth_token_provider': TokenProvider()\n",
        "    }\n",
        "    producer = KafkaProducer(**config)  # Use keyword unpacking for clear configuration\n",
        "\n",
        "    # Calculate total distance and time\n",
        "    start = (starting_longitude, starting_latitude)  # Swap order for consistency with haversine_distance\n",
        "    end = (ending_longitude, ending_latitude)  # Swap order for consistency with haversine_distance\n",
        "    total_distance = geodesic(start, end).meters if geodesic else haversine_distance(*start, *end)\n",
        "    total_time = total_distance / speed_meters_per_second\n",
        "\n",
        "    # Generate data points\n",
        "    generate_data_time = 1  # seconds\n",
        "    num_points = int(total_time / generate_data_time) + 1  # Include start and end points\n",
        "\n",
        "    prior_latitude = starting_latitude\n",
        "    prior_longitude = starting_longitude\n",
        "\n",
        "    for i in range(num_points):\n",
        "        fraction = i / (num_points - 1)  # Normalize fraction for even distribution\n",
        "        lat = starting_latitude + fraction * (ending_latitude - starting_latitude)\n",
        "        lon = starting_longitude + fraction * (ending_longitude - starting_longitude)\n",
        "        distance_to_destination = haversine_distance(lat, lon, ending_latitude, ending_longitude)\n",
        "        map_url = f\"https://www.google.com/maps/place/{lat},{lon}/@{lat},{lon},17z\"\n",
        "\n",
        "        # Kafka\n",
        "        # Log\n",
        "        # user, event_time, current_lat, current_long, starting_lat, starting_long, dest_lat, dest_long, walking_speed_meters_per_second, distance_to_destination, map_url\n",
        "        message_data = {\n",
        "            \"customer_geo_location_id\" : f\"{uuid.uuid4()}\",\n",
        "            \"customer_id\": customer_id,\n",
        "            \"event_timestamp_millis\": int(time.time() * 1000),\n",
        "            \"prior_latitude\": prior_latitude,\n",
        "            \"prior_longitude\": prior_longitude,\n",
        "            \"current_latitude\": lat,\n",
        "            \"current_longitude\": lon,\n",
        "            \"debug_destination_latitude\": ending_latitude,\n",
        "            \"debug_destination_longitude\": ending_longitude,\n",
        "            \"debug_walking_speed_mps\": speed_meters_per_second,\n",
        "            \"debug_map_url\" : f\"{map_url}\"\n",
        "        }\n",
        "\n",
        "        # Save for next interation\n",
        "        prior_latitude = lat\n",
        "        prior_longitude = lon\n",
        "\n",
        "        # Serialize data to bytes\n",
        "        serialized_data = json.dumps(message_data).encode('utf-8')\n",
        "\n",
        "        # Define the key based on your needs (e.g., customer_id)\n",
        "        key = str(customer_id).encode('utf-8')\n",
        "\n",
        "        # Produce the message with key\n",
        "        producer.send(kafka_topic_name, key=key, value=serialized_data) # callback=delivery_callback\n",
        "\n",
        "        if i % 100 == 0:\n",
        "          print(f\"{customer_name} distance to go ({distance_to_destination:.2f} km): {map_url}\")\n",
        "          print(f\"message_data: {message_data}\")\n",
        "          producer.flush()\n",
        "          if just_one_record:\n",
        "            return # for testing\n",
        "\n",
        "        time.sleep(generate_data_time)\n",
        "\n",
        "    producer.flush()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYQO1IKpXwyY"
      },
      "source": [
        "##### simulate_walk_confluent_kafka_producer (Confluent Kafka Producer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIL3cop8Xwyg"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from geopy.distance import geodesic\n",
        "import urllib.parse\n",
        "import uuid\n",
        "\n",
        "def simulate_walk_confluent_kafka_producer(customer_id, customer_name, starting_latitude, starting_longitude, \\\n",
        "                  ending_latitude, ending_longitude, speed_meters_per_second=1.4, just_one_record = False):\n",
        "    \"\"\"\n",
        "    Simulates a walk from a starting point to an ending point.\n",
        "\n",
        "    Args:\n",
        "        customer_name (str): Name of the person walking.\n",
        "        starting_latitude (float): Starting latitude in degrees.\n",
        "        starting_longitude (float): Starting longitude in degrees.\n",
        "        ending_latitude (float): Ending latitude in degrees.\n",
        "        ending_longitude (float): Ending longitude in degrees.\n",
        "        speed_meters_per_second (float, optional): Walking speed in meters per second. Defaults to 1.4.\n",
        "\n",
        "    Prints information about the walk at regular intervals.\n",
        "    \"\"\"\n",
        "    import confluent_kafka\n",
        "    import functools\n",
        "\n",
        "    # Kafka Producer configuration with SASL_PLAIN authentication\n",
        "    # This requires a service principal key (json file) which must be base64 encoded\n",
        "    # !gsutil cp gs://bucket-name/your-key.json sa.key.json <- This assumes you exported the key to a bucket\n",
        "    # secret = !(cat sa.key.json | base64 -w 0)\n",
        "    # secret = secret[0]\n",
        "    #config = {\n",
        "    #    'bootstrap.servers': f'bootstrap.{kafka_cluster_name}.{region}.managedkafka.{project_id}.cloud.goog',\n",
        "    #    'sasl.username':     f'kafka-sp@{project_id}.iam.gserviceaccount.com',\n",
        "    #    'sasl.password':     secret,\n",
        "    #    'security.protocol': 'SASL_SSL',\n",
        "    #    'sasl.mechanisms':   'PLAIN',\n",
        "    #    'acks':              'all'\n",
        "    #}\n",
        "\n",
        "\n",
        "    # Kafka Producer configuration with OAUTHBEARER authentication\n",
        "    # https://docs.confluent.io/platform/current/clients/confluent-kafka-python/html/index.html#pythonclient-configuration\n",
        "    # https://github.com/confluentinc/confluent-kafka-python/blob/master/examples/oauth_producer.py\n",
        "    config = {\n",
        "        'bootstrap.servers': f'bootstrap.{kafka_cluster_name}.{region}.managedkafka.{project_id}.cloud.goog',\n",
        "        'security.protocol': 'SASL_SSL',\n",
        "        'sasl.mechanisms': 'OAUTHBEARER',\n",
        "        'oauth_cb': functools.partial(ConfluentTokenProvider, None),\n",
        "        #'oauthbearer_token_refresh_cb': functools.partial(ConfluentTokenProvider, None),\n",
        "        'error_cb' : functools.partial(ConfluentErrorProvider),\n",
        "        'acks': 'all'\n",
        "    }\n",
        "\n",
        "    producer = confluent_kafka.Producer(config)\n",
        "\n",
        "    # Calculate total distance and time\n",
        "    start = (starting_longitude, starting_latitude)  # Swap order for consistency with haversine_distance\n",
        "    end = (ending_longitude, ending_latitude)  # Swap order for consistency with haversine_distance\n",
        "    total_distance = geodesic(start, end).meters if geodesic else haversine_distance(*start, *end)\n",
        "    total_time = total_distance / speed_meters_per_second\n",
        "\n",
        "    # Generate data points\n",
        "    generate_data_time = 1  # seconds\n",
        "    num_points = int(total_time / generate_data_time) + 1  # Include start and end points\n",
        "\n",
        "    prior_latitude = starting_latitude\n",
        "    prior_longitude = starting_longitude\n",
        "\n",
        "    for i in range(num_points):\n",
        "        fraction = i / (num_points - 1)  # Normalize fraction for even distribution\n",
        "        lat = starting_latitude + fraction * (ending_latitude - starting_latitude)\n",
        "        lon = starting_longitude + fraction * (ending_longitude - starting_longitude)\n",
        "        distance_to_destination = haversine_distance(lat, lon, ending_latitude, ending_longitude)\n",
        "        map_url = f\"https://www.google.com/maps/place/{lat},{lon}/@{lat},{lon},17z\"\n",
        "\n",
        "        # Kafka\n",
        "        # Log\n",
        "        # user, event_time, current_lat, current_long, starting_lat, starting_long, dest_lat, dest_long, walking_speed_meters_per_second, distance_to_destination, map_url\n",
        "        message_data = {\n",
        "            \"customer_geo_location_id\" : f\"{uuid.uuid4()}\",\n",
        "            \"customer_id\": customer_id,\n",
        "            \"event_timestamp_millis\": int(time.time() * 1000),\n",
        "            \"prior_latitude\": prior_latitude,\n",
        "            \"prior_longitude\": prior_longitude,\n",
        "            \"current_latitude\": lat,\n",
        "            \"current_longitude\": lon,\n",
        "            \"debug_destination_latitude\": ending_latitude,\n",
        "            \"debug_destination_longitude\": ending_longitude,\n",
        "            \"debug_walking_speed_mps\": speed_meters_per_second,\n",
        "            \"debug_map_url\" : f\"{map_url}\"\n",
        "        }\n",
        "\n",
        "        # Save for next interation\n",
        "        prior_latitude = lat\n",
        "        prior_longitude = lon\n",
        "\n",
        "\n",
        "        # Serialize data to bytes\n",
        "        serialized_data = json.dumps(message_data).encode('utf-8')\n",
        "\n",
        "        # Define the key based on your needs (e.g., customer_id)\n",
        "        key = str(customer_id).encode('utf-8')\n",
        "\n",
        "        # Produce the message with key\n",
        "        producer.produce(kafka_topic_name, key=key, value=serialized_data) # callback=delivery_callback\n",
        "\n",
        "        if i % 100 == 0:\n",
        "          print(f\"{customer_name} distance to go ({distance_to_destination:.2f} km): {map_url}\")\n",
        "          print(f\"message_data: {message_data}\")\n",
        "          producer.flush()\n",
        "          if just_one_record:\n",
        "            return # for testing\n",
        "\n",
        "        time.sleep(generate_data_time)\n",
        "\n",
        "    producer.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gtEd0M7rkfc"
      },
      "source": [
        "#### Create Simulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QassjFmybtao"
      },
      "source": [
        "##### create_people"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diPJ-aqQMwhz"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "\n",
        "def create_people(starting_customer_id, number_of_people, producer_method_name = simulate_walk_open_source_kafka_producer) -> None:\n",
        "  \"\"\"\n",
        "  Calculates people who will walk towards various locations.  This will generate a thread for each person\n",
        "  and simulate them walking.\n",
        "\n",
        "  Args:\n",
        "    number_of_people: The number of people to generate.\n",
        "    producer_method_name: The method to use to simulate the walk.\n",
        "\n",
        "  Returns:\n",
        "    None\n",
        "  \"\"\"\n",
        "  location_1_latitude, location_1_longitude = 51.501959717656455, -0.1411590270077308  # The British Museum\n",
        "  location_2_latitude, location_2_longitude = 51.5081747307638,   -0.07596870182999621 # Tower of London\n",
        "\n",
        "  # Create a box where we will generate random people starting locations.\n",
        "  # This is 10 km from The British Museum\n",
        "  bounding_box_min_lat, bounding_box_max_lat, bounding_box_min_lon, bounding_box_max_lon = bounding_box(location_1_latitude, location_1_longitude, 10)\n",
        "\n",
        "  people = []\n",
        "  for i in range(number_of_people):\n",
        "    print(f\"Generating Person: {i+starting_customer_id}\")\n",
        "    person_dict = {\n",
        "          \"customer_id\": i+starting_customer_id,\n",
        "          \"name\": f\"person {i+starting_customer_id}\",\n",
        "          \"starting_latitude\":  random.uniform(bounding_box_min_lat, bounding_box_max_lat),\n",
        "          \"starting_longitude\": random.uniform(bounding_box_min_lon, bounding_box_max_lon),\n",
        "          \"destination_latitude\": location_1_latitude if i % 2 == 0 else location_2_latitude,\n",
        "          \"destination_longitude\": location_1_longitude if i % 2 == 0 else location_2_longitude,\n",
        "          \"walking_speed_meters_per_second\": round(random.uniform(1, 5),2) # The average walking speed of a person is approximately 1.4 meters per second.\n",
        "    }\n",
        "    distance_to_destination = haversine_distance(person_dict[\"starting_latitude\"], person_dict[\"starting_longitude\"],\\\n",
        "                                                 person_dict[\"destination_latitude\"], person_dict[\"destination_longitude\"])\n",
        "    print(distance_to_destination)\n",
        "    people.append(person_dict)\n",
        "\n",
        "  threads = []\n",
        "  for item in people:\n",
        "    threads.append(threading.Thread(target=producer_method_name, args=(item[\"customer_id\"], item[\"name\"], \\\n",
        "                    item[\"starting_latitude\"], item[\"starting_longitude\"], \\\n",
        "                    item[\"destination_latitude\"], item[\"destination_longitude\"], \\\n",
        "                    item[\"walking_speed_meters_per_second\"])))\n",
        "\n",
        "  for thread in threads:\n",
        "    thread.start()\n",
        "\n",
        "  for thread in threads:\n",
        "    thread.join()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1NflkRlaebN"
      },
      "source": [
        "##### Clear the table so we can see our results (easily)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiTxYrZ2aYv5"
      },
      "outputs": [],
      "source": [
        "#%%bigquery\n",
        "\n",
        "#TRUNCATE TABLE `chocolate_ai.customer_geo_location`;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5LcAR7xZkK8"
      },
      "source": [
        "##### Test a single record from Open Source and Confluent Producers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMwCnolZZ8n2"
      },
      "outputs": [],
      "source": [
        "# Open Source Kafka Producer\n",
        "Buckingham_Palace_latitude, Buckingham_Palace_longitude = 51.501959717656455, -0.1411590270077308\n",
        "London_Bridge_latitude, London_Bridge_longitude = 51.5081747307638, -0.07596870182999621\n",
        "\n",
        "print(\"Open Source Kafka Producer\")\n",
        "simulate_walk_open_source_kafka_producer(9999, \"Person Open Source\", Buckingham_Palace_latitude, Buckingham_Palace_longitude, \\\n",
        "              London_Bridge_latitude, London_Bridge_longitude, speed_meters_per_second=100, just_one_record = True)\n",
        "print()\n",
        "\n",
        "print(\"Confluent Kafka Producer\")\n",
        "# Confluent Source Kafka Producer\n",
        "simulate_walk_confluent_kafka_producer(8888, \"Person Confluent\", Buckingham_Palace_latitude, Buckingham_Palace_longitude, \\\n",
        "              London_Bridge_latitude, London_Bridge_longitude, speed_meters_per_second=200, just_one_record = True)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8FOmtB4Zoip"
      },
      "outputs": [],
      "source": [
        "%%bigquery\n",
        "\n",
        "-- It might take a second to see the data\n",
        "select * from `chocolate_ai.customer_geo_location` order by customer_id DESC, event_timestamp_millis LIMIT 100;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtWt7RKRgrNV"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Open Source Kafka Producer\n",
        "Person Open Source distance to go (4.56 km): https://www.google.com/maps/place/51.501959717656455,-0.1411590270077308/@51.501959717656455,-0.1411590270077308,17z\n",
        "message_data: {'customer_id': 9999, 'event_timestamp_millis': 1726065315461, 'current_latitude': 51.501959717656455, 'current_longitude': -0.1411590270077308, 'starting_latitude': 51.501959717656455, 'starting_longitude': -0.1411590270077308, 'destination_latitude': 51.509391, 'destination_longitude': -0.0764338, 'walking_speed_mps': 100, 'distance_to_dest_km': 4.555325295137807}\n",
        "\n",
        "\"\"\"\n",
        "Buckingham_Palace_latitude, Buckingham_Palace_longitude = 51.501959717656455, -0.1411590270077308\n",
        "London_Bridge_latitude, London_Bridge_longitude = 51.5081747307638, -0.07596870182999621\n",
        "\n",
        "print(haversine_distance(London_Bridge_latitude, London_Bridge_longitude, Buckingham_Palace_latitude, Buckingham_Palace_longitude))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk41BjwKZyIp"
      },
      "source": [
        "##### Start a thread and create many people walking **(this will generate lots of data)**\n",
        "- NOTE:\n",
        "  - You must **uncomment** out one of the below lines.  This way if you accidently run the cell you do not kick off lots of data.\n",
        "  - Do not run both since they both start with customer #1 and you would have overlapping routes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7cqpG9bONaL"
      },
      "outputs": [],
      "source": [
        "# Create some people and send using Open Source Producer\n",
        "create_people(101, 500, simulate_walk_open_source_kafka_producer)\n",
        "\n",
        "# Create some people and send using Confluent Kafka Producer\n",
        "# create_people(1, 500, simulate_walk_confluent_kafka_producer)\n",
        "\n",
        "# NOTES:\n",
        "# 1. If you are running this for a long time, you should right click this cell and clear the output.\n",
        "# 2. The people are walking straght towards the destination, they will walk over water and through buildings.\n",
        "#    Google Maps routing could be called for a realistic walking route."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wS1pzegZhuF"
      },
      "outputs": [],
      "source": [
        "%%bigquery\n",
        "\n",
        "-- How many records\n",
        "SELECT COUNT(*) FROM `data-analytics-preview.chocolate_ai.customer_geo_location`;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzUojEJfZj3C"
      },
      "outputs": [],
      "source": [
        "%%bigquery\n",
        "\n",
        "-- Events per second (add day if you are running accross days)\n",
        "SELECT EXTRACT(HOUR FROM TIMESTAMP_MILLIS(event_timestamp_millis)) AS Hour,\n",
        "       EXTRACT(MINUTE FROM TIMESTAMP_MILLIS(event_timestamp_millis)) AS Minute,\n",
        "       EXTRACT(SECOND FROM TIMESTAMP_MILLIS(event_timestamp_millis)) AS Second,\n",
        "       COUNT(*) AS Cnt\n",
        "  FROM `data-analytics-preview.chocolate_ai.customer_geo_location`\n",
        "GROUP BY ALL\n",
        "ORDER BY 1 DESC, 2 DESC, 3 DESC\n",
        "LIMIT 10;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvKsSGLlZd8t"
      },
      "outputs": [],
      "source": [
        "%%bigquery\n",
        "\n",
        "-- See which customers have broken the geofence\n",
        "WITH raw_data AS (\n",
        "  SELECT *\n",
        "  FROM `data-analytics-preview.chocolate_ai.customer_geo_location`\n",
        ")\n",
        ", geo_data AS (\n",
        "  SELECT *,\n",
        "         ST_DISTANCE(\n",
        "          ST_GEOGPOINT(prior_longitude, prior_latitude),\n",
        "          ST_GEOGPOINT(current_longitude, current_latitude)\n",
        "         ) AS meters_travel_since_prior_distance,\n",
        "         ST_DISTANCE(\n",
        "          ST_GEOGPOINT(current_longitude, current_latitude),\n",
        "          ST_GEOGPOINT(debug_destination_longitude, debug_destination_latitude)\n",
        "         ) AS meters_to_dest_distance,\n",
        "         ST_DISTANCE(\n",
        "          ST_GEOGPOINT(current_longitude, current_latitude),\n",
        "          ST_GEOGPOINT(debug_destination_longitude, debug_destination_latitude)\n",
        "         ) / 1000 AS km_to_dest_distance,\n",
        "  FROM raw_data\n",
        ")\n",
        ", results AS (\n",
        "  SELECT *,\n",
        "         CASE WHEN meters_to_dest_distance > 1000\n",
        "               AND meters_to_dest_distance - meters_travel_since_prior_distance < 1000\n",
        "              THEN TRUE\n",
        "              ELSE FALSE\n",
        "          END AS entered_geofence\n",
        "  FROM geo_data\n",
        ")\n",
        "SELECT TO_JSON_STRING(STRUCT(customer_geo_location_id,\n",
        "                             customer_id,\n",
        "                             current_latitude,\n",
        "                             current_longitude,\n",
        "                             km_to_dest_distance,\n",
        "                             debug_map_url)) AS message,\n",
        "      TO_JSON(STRUCT(CAST(TIMESTAMP_MILLIS(event_timestamp_millis) AS STRING) AS event_timestamp)) AS _ATTRIBUTES\n",
        "  FROM results\n",
        "  where entered_geofence = true;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmrudJXifsbq"
      },
      "outputs": [],
      "source": [
        "%%bigquery\n",
        "\n",
        "-- User Input: Change the below job_id (you get this from the continuous query | job information tab)\n",
        "-- The number or slots the continuous query\n",
        "SELECT job.creation_time,\n",
        "      job.project_id,\n",
        "      job.project_number,\n",
        "      job.user_email,\n",
        "      job.job_id,\n",
        "      job.job_type,\n",
        "      job.statement_type,\n",
        "      job.priority,\n",
        "      job.start_time,\n",
        "      job.end_time,\n",
        "      job.query,\n",
        "      job.state,\n",
        "      job.reservation_id,\n",
        "      job.total_bytes_processed,\n",
        "      job.total_slot_ms,\n",
        "      job.error_result.reason     AS error_result_reason,\n",
        "      job.error_result.location   AS error_result_location,\n",
        "      job.error_result.debug_info AS error_result_debug_info,\n",
        "      job.error_result.message    AS error_result_message,\n",
        "\n",
        "      -- Average slot utilization per job is calculated by dividing\n",
        "      -- total_slot_ms by the millisecond duration of the job\n",
        "      CAST(SAFE_DIVIDE(job.total_slot_ms,(TIMESTAMP_DIFF(IFNULL(job.end_time,CURRENT_TIMESTAMP()), job.start_time, MILLISECOND))) AS FLOAT64) AS job_avg_slots\n",
        "\n",
        "  FROM `data-analytics-preview`.`region-us`.INFORMATION_SCHEMA.JOBS AS job\n",
        "        CROSS JOIN UNNEST(job.job_stages) as unnest_job_stages\n",
        "        CROSS JOIN UNNEST(job.timeline) AS unnest_timeline\n",
        "  WHERE job.job_id = 'bquxjob_544ac149_191ec5b33ed'\n",
        "GROUP BY ALL;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOdfoK4abe5G"
      },
      "source": [
        "### Consume data using Kafka Consumers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39tMb3BRbDB9"
      },
      "source": [
        "#### Open Source Kafka Consumer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfrUFAPSbJZt"
      },
      "outputs": [],
      "source": [
        "def openSourceKafkaConsumer():\n",
        "  from kafka import KafkaConsumer\n",
        "\n",
        "  # Kafka Consumer configuration with SASL_PLAIN authentication\n",
        "  # This requires a service principal key (json file) which must be base64 encoded\n",
        "  # !gsutil cp gs://bucket-name/your-key.json sa.key.json <- This assumes you exported the key to a bucket\n",
        "  # secret = !(cat sa.key.json | base64 -w 0)\n",
        "  # secret = secret[0]\n",
        "  #config = {\n",
        "  #    'bootstrap_servers': f'bootstrap.{kafka_cluster_name}.{region}.managedkafka.{project_id}.cloud.goog',\n",
        "  #    'security_protocol': 'SASL_SSL',  # Use SASL_PLAINTEXT for username/password\n",
        "  #    'sasl_mechanism': 'PLAIN',\n",
        "  #    'sasl_plain_username': f'kafka-sp@{project_id}.iam.gserviceaccount.com',\n",
        "  #    'sasl_plain_password': secret,\n",
        "  #    'group_id':          'kafka-group-id',\n",
        "  #    'auto_offset_reset': 'earliest'\n",
        "  #}\n",
        "\n",
        "  # Kafka Consumer configuration with OAUTHBEARER authentication\n",
        "  config = {\n",
        "      'bootstrap_servers': f'bootstrap.{kafka_cluster_name}.{region}.managedkafka.{project_id}.cloud.goog',\n",
        "      'security_protocol': 'SASL_SSL',\n",
        "      'sasl_mechanism': 'OAUTHBEARER',\n",
        "      'sasl_oauth_token_provider': TokenProvider(),\n",
        "      'group_id': 'kafka-group-id',\n",
        "      'auto_offset_reset': 'earliest'\n",
        "  }\n",
        "\n",
        "  # Create Consumer instance\n",
        "  consumer = KafkaConsumer(**config)  # Use keyword unpacking for clear configuration\n",
        "\n",
        "  # Subscribe to topic\n",
        "  consumer.subscribe([kafka_topic_name])\n",
        "\n",
        "  i = 0\n",
        "  max_items = 50\n",
        "\n",
        "  # Poll for new messages from Kafka and print them.\n",
        "  try:\n",
        "      while True:\n",
        "          messages = consumer.poll(1.0)\n",
        "          for partition, messages in messages.items():\n",
        "              for message in messages:\n",
        "                  i += 1\n",
        "                  if i >= max_items:\n",
        "                      print(f\"Reached max items ({max_items})\")\n",
        "                      break\n",
        "                  try:\n",
        "                      print(f\"Consumed record with key {message.key} and value {message.value}\")\n",
        "                      # Process the message here (e.g., parse JSON, store data)\n",
        "                      message_data = json.loads(message.value)\n",
        "                      print(message_data)\n",
        "                  except Exception as e:\n",
        "                      print(f\"Error processing message: {e}\")\n",
        "              if i >= max_items:\n",
        "                  break\n",
        "          if i >= max_items:\n",
        "              break\n",
        "\n",
        "  except KeyboardInterrupt:\n",
        "      pass\n",
        "  finally:\n",
        "      # Leave group and commit final offsets\n",
        "      consumer.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnq1zz97cyg3"
      },
      "outputs": [],
      "source": [
        "openSourceKafkaConsumer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRxIHtJNZLoZ"
      },
      "source": [
        "#### Confluent Source Kafka Consumer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWrn9XYWZNwK"
      },
      "outputs": [],
      "source": [
        "def confluentKafkaConsumer():\n",
        "  from confluent_kafka import Consumer\n",
        "  import functools\n",
        "\n",
        "  # Kafka Consumer configuration with SASL_PLAIN authentication\n",
        "  # This requires a service principal key (json file) which must be base64 encoded\n",
        "  # !gsutil cp gs://bucket-name/your-key.json sa.key.json <- This assumes you exported the key to a bucket\n",
        "  # secret = !(cat sa.key.json | base64 -w 0)\n",
        "  # secret = secret[0]\n",
        "  #config = {\n",
        "  #    # User-specific properties that you must set\n",
        "  #    'bootstrap.servers': f'bootstrap.{kafka_cluster_name}.{region}.managedkafka.{project_id}.cloud.goog',\n",
        "  #    'sasl.username':     f'kafka-sp@{project_id}.iam.gserviceaccount.com',\n",
        "  #    'sasl.password':     secret,#\n",
        "  #    'security.protocol': 'SASL_SSL',\n",
        "  #    'sasl.mechanisms':   'PLAIN',\n",
        "  #    'group.id':          'kafka-group-id',\n",
        "  #    'auto.offset.reset': 'earliest'\n",
        "  #}\n",
        "\n",
        "  # Kafka Consumer configuration with OAUTHBEARER authentication\n",
        "  config = {\n",
        "      'bootstrap.servers': f'bootstrap.{kafka_cluster_name}.{region}.managedkafka.{project_id}.cloud.goog',\n",
        "      'security.protocol': 'SASL_SSL',\n",
        "      'sasl.mechanisms': 'OAUTHBEARER',\n",
        "      'oauth_cb': functools.partial(ConfluentTokenProvider, None),\n",
        "      'error_cb' : functools.partial(ConfluentErrorProvider),\n",
        "      'group.id': 'kafka-group-id',\n",
        "      'auto.offset.reset': 'earliest'\n",
        "  }\n",
        "\n",
        "  # Create Consumer instance\n",
        "  consumer = Consumer(config)\n",
        "\n",
        "  # Subscribe to topic\n",
        "  consumer.subscribe([kafka_topic_name])\n",
        "\n",
        "  i = 0\n",
        "  max_items = 50\n",
        "\n",
        "  # Poll for new messages from Kafka and print them.\n",
        "  try:\n",
        "      while True:\n",
        "          msg = consumer.poll(1.0)\n",
        "          if msg is None:\n",
        "              # Initial message consumption may take up to\n",
        "              # `session.timeout.ms` for the consumer group to\n",
        "              # rebalance and start consuming\n",
        "              print(\"Waiting...\")\n",
        "          elif msg.error():\n",
        "              print(\"ERROR: %s\".format(msg.error()))\n",
        "          else:\n",
        "              # Extract the (optional) key and value, and print.\n",
        "              i += 1\n",
        "              print(msg.value().decode('utf-8'))\n",
        "              if i >= max_items:\n",
        "                  print(f\"Reached max items ({max_items})\")\n",
        "                  break\n",
        "  except KeyboardInterrupt:\n",
        "      pass\n",
        "  finally:\n",
        "      # Leave group and commit final offsets\n",
        "      consumer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYlnqb3hZQRX"
      },
      "outputs": [],
      "source": [
        "confluentKafkaConsumer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42IxhtRRrvR-"
      },
      "source": [
        "### Clean Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lF2Z7skFbvf"
      },
      "outputs": [],
      "source": [
        "# Note if you do not know your job id, or overwrote the value, click here to open and manually Cancel the job\n",
        "# https://console.cloud.google.com/dataflow/jobs\n",
        "\n",
        "user_input = input(f\"Do you want to delete your DataFlow Job {jobName} (Y/n)?\")\n",
        "if user_input == \"Y\":\n",
        "  stopDataflowJobApacheKafkaToBigQuery(jobName)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qj52V67PrxiX"
      },
      "outputs": [],
      "source": [
        "user_input = input(\"Do you want to delete your Apache Kafka for BigQuery (Y/n)?\")\n",
        "if user_input == \"Y\":\n",
        "  deleteApacheKafkaForBigQueryCluster()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASQ2BPisXDA0"
      },
      "source": [
        "### Reference Links"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uuW4Aw27Xnt"
      },
      "source": [
        "- [Python Kafka Libary](https://kafka-python.readthedocs.io/en/master/index.html)\n",
        "- [Confluent Python Library](https://docs.confluent.io/platform/current/clients/confluent-kafka-python/html/index.html)\n",
        "- [Google Apache Kafka for BigQuery - Authentication](https://cloud.google.com/managed-kafka/docs/authentication-kafka#oauthbearer)\n",
        "- [Google Apache Kafka for BigQuery - OAuth Sample Code](https://cloud.google.com/managed-kafka/docs/authentication-kafka#oauthbearer)\n",
        "* [Google Apache Kafka for BigQuery - Sample Code](https://github.com/googleapis/managedkafka)\n",
        "- [Confluent - Sample OAuth Code](https://github.com/confluentinc/confluent-kafka-python/blob/master/examples/oauth_producer.py)\n",
        "* [Confluent - Kafka Config for Python](https://docs.confluent.io/platform/current/clients/confluent-kafka-python/html/index.html#pythonclient-configuration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qz3RoOJz9oDD"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "  time.sleep(30)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "HMsUvoF4BP7Y",
        "m65vp54BUFRi",
        "UmyL-Rg4Dr_f",
        "sZ6m_wGrK0YG",
        "JbOjdSP1kN9T",
        "lkj0u39akoZu",
        "fo38SP8YuFdb",
        "WUwyHLMhuYws",
        "A84k91wbujTD",
        "GNs0zbGokXNQ",
        "XiketZsXkdRe",
        "k9p6kRH4F6vv",
        "IrARsGGPpbsc",
        "ftO94aBR-t1O",
        "v_x4X8RSNMJt",
        "AlDNFYg_b6Do",
        "ITnZPgO_b9m2",
        "etIgADeob6FF",
        "s1NflkRlaebN",
        "iOdfoK4abe5G",
        "aRxIHtJNZLoZ",
        "42IxhtRRrvR-",
        "ASQ2BPisXDA0"
      ],
      "name": "Campaign-Performance-Geofencing-Simulation.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
